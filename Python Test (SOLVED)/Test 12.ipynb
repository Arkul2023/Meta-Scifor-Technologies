{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "865ce599",
   "metadata": {},
   "source": [
    "## MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfb4125-1ce6-4b9c-97c4-6d1696462674",
   "metadata": {},
   "source": [
    "##### What is MLflow primarily used for?\n",
    "\n",
    "    - a) Data preprocessing\n",
    "    - b) Model training\n",
    "    - c) Machine learning lifecycle management\n",
    "    - d) Web development\n",
    "\n",
    "ANS- c) Machine learning lifecycle management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3c3eae-1e26-4578-a73c-d4d7614d6aa3",
   "metadata": {},
   "source": [
    "##### Which component of MLflow is used to track experiments?\n",
    "    - a) MLflow Tracking\n",
    "    - b) MLflow Projects\n",
    "    - c) MLflow Models\n",
    "    - d) MLflow Registry\n",
    "\n",
    "ANS - a) MLflow Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed72965-7c28-45af-85f2-0d25096498e4",
   "metadata": {},
   "source": [
    "###### How can MLflow be integrated with popular ML frameworks?\n",
    "    a) Through APIs and autologging\n",
    "    b) Using SQL queries\n",
    "    c) Only through manual logging\n",
    "    d) By writing custom scripts\n",
    "\n",
    "ANS - a) Through APIs and autologging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b03288-97e4-4445-a3ea-0b7ab5a1855f",
   "metadata": {},
   "source": [
    "###### MLflow allows logging which of the following?\n",
    "    a) Metrics\n",
    "    b) Parameters\n",
    "    c) Artifacts\n",
    "    d) All of the above\n",
    "\n",
    "ANS - d) All of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14bb501-8aec-4715-993b-4eeb6961fcec",
   "metadata": {},
   "source": [
    "###### Which command is used to start the MLflow UI?\n",
    "    a) mlflow start-ui\n",
    "    b) mlflow ui\n",
    "    c) mlflow launch-ui\n",
    "    d) mlflow dashboard\n",
    "\n",
    "ANS - b) mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb1d8e-e402-42e1-a959-f6170338b09c",
   "metadata": {},
   "source": [
    "###### What file format does MLflow use to store model metadata?\n",
    "    a) JSON\n",
    "    b) YAML\n",
    "    c) CSV\n",
    "    d) XML\n",
    "\n",
    "ANS - a) JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5d05b9",
   "metadata": {},
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223cdb87-55c1-4e0b-a79b-564212909628",
   "metadata": {},
   "source": [
    "###### TensorBoard is mainly used for:\n",
    "    a) Model deployment\n",
    "    b) Experiment visualization\n",
    "    c) Data collection\n",
    "    d) Image processing\n",
    "\n",
    "ANS - b) Experiment visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f18c7-7a9d-4f43-83da-ca84c0b9cb71",
   "metadata": {},
   "source": [
    "###### What types of data can TensorBoard visualize?\n",
    "    a) Scalars\n",
    "    b) Images\n",
    "    c) Histograms\n",
    "    d) All of the above\n",
    "\n",
    "ANS - d) All of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d381d0-b180-431f-9696-fcc75c5abf62",
   "metadata": {},
   "source": [
    "###### Which command is used to launch TensorBoard?\n",
    "    a) tensorboard --logdir=<path>\n",
    "    b) tensorboard start\n",
    "    c) tensorboard run\n",
    "    d) tensorboard launch\n",
    "\n",
    "ANS - a) tensorboard --logdir=<path>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc96d9e-6c1e-40c1-8396-6cd5b6960186",
   "metadata": {},
   "source": [
    "###### TensorBoard logs data in which directory format?\n",
    "    a) .tensorboard\n",
    "    b) ./logs/\n",
    "    c) .logdir\n",
    "    d) ./output/\n",
    "\n",
    "ANS -  b) ./logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d674c940-c3bf-4472-bf29-1ab0f1ffc63d",
   "metadata": {},
   "source": [
    "###### In TensorBoard, how are logs typically stored?\n",
    "    a) SQL Database\n",
    "    b) Event files\n",
    "    c) JSON files\n",
    "    d) CSV files\n",
    "\n",
    "ANS - b) Event files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959ba01d-e3d7-4448-8c74-1d395646d8f8",
   "metadata": {},
   "source": [
    "##### Which ML framework primarily integrates with TensorBoard?\n",
    "    a) PyTorch\n",
    "    b) TensorFlow\n",
    "    c) Scikit-learn\n",
    "    d) XGBoost\n",
    "\n",
    "ANS - b) TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98001b4",
   "metadata": {},
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223ef5b5-d809-4b9a-bf6b-291135d0a73e",
   "metadata": {},
   "source": [
    "###### What is cloud computing?\n",
    "    a) A programming language\n",
    "    b) A storage device\n",
    "    c) On-demand delivery of computing services over the internet\n",
    "    d) A networking protocol\n",
    "\n",
    "ANS - c) On-demand delivery of computing services over the internet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c9ce84-19f2-42be-afbe-f0c4d71ab6e4",
   "metadata": {},
   "source": [
    "###### Which of the following is not a cloud computing model?\n",
    "    a) IaaS\n",
    "    b) PaaS\n",
    "    c) SaaS\n",
    "    d) HTTP\n",
    "\n",
    "ANS - d) HTTP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e0f24-96c7-457a-b34c-b04144da2b91",
   "metadata": {},
   "source": [
    "###### AWS, Google Cloud, and Azure provide which type of cloud service?\n",
    "    a) Private cloud\n",
    "    b) Public cloud\n",
    "    c) Hybrid cloud\n",
    "    d) On-premises cloud\n",
    "\n",
    "ANS - b) Public cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5d1a19-73b3-4153-a1ee-e771f03f4b6f",
   "metadata": {},
   "source": [
    "##### Which cloud computing model provides the highest level of user control?\n",
    "    a) SaaS\n",
    "    b) PaaS\n",
    "    c) IaaS\n",
    "    d) FaaS\n",
    "\n",
    "ANS - c) IaaS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93df8091-a679-42ff-9fd9-b831c2d58cbd",
   "metadata": {},
   "source": [
    "##### Which cloud service provider offers Lambda for serverless computing?\n",
    "    a) Google Cloud\n",
    "    b) AWS\n",
    "    c) Azure\n",
    "    d) IBM Cloud\n",
    "\n",
    "ANS - b) AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f843cfea-1066-43c2-bb21-b7025a5db3ff",
   "metadata": {},
   "source": [
    "###### What does \"elasticity\" in cloud computing refer to?\n",
    "    a) Automatic scaling of resources\n",
    "    b) Data compression\n",
    "    c) Network security\n",
    "    d) Storage encryption\n",
    "\n",
    "ANS - a) Automatic scaling of resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6fc5cc",
   "metadata": {},
   "source": [
    "## Flask "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c72f58-834f-4d61-a2d3-eacdb506f019",
   "metadata": {},
   "source": [
    "##### Flask is a framework for which programming language?\n",
    "    a) Java\n",
    "    b) Python\n",
    "    c) C++\n",
    "    d) JavaScript\n",
    "\n",
    "ANS - b) Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99296e5b-c6bb-45d5-9cb2-3f0898d29100",
   "metadata": {},
   "source": [
    "##### Which function is used to define a route in Flask?\n",
    "    a) flask.route()\n",
    "    b) app.route()\n",
    "    c) route.app()\n",
    "    d) flask.define_route()\n",
    "\n",
    "ANS - b) app.route()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befd4aa6-fecc-4578-b1db-515fd0f3cf06",
   "metadata": {},
   "source": [
    "##### What is the default port for Flask applications?\n",
    "    a) 5000\n",
    "    b) 8000\n",
    "    c) 8080\n",
    "    d) 3000\n",
    "\n",
    "ANS - a) 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da5feb-3280-4966-9922-76b93b70a3ce",
   "metadata": {},
   "source": [
    "##### How do you run a Flask application?\n",
    "    a) python app.py\n",
    "    b) flask run\n",
    "    c) python -m flask run\n",
    "    d) All of the above\n",
    "\n",
    "ANS - a) python app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3402cbfb-ec17-4487-b81d-6163bbd0fa74",
   "metadata": {},
   "source": [
    "##### What is the purpose of Flask-SQLAlchemy?\n",
    "    a) Web scraping\n",
    "    b) ORM for Flask applications\n",
    "    c) Flask UI development\n",
    "    d) API testing\n",
    "\n",
    "ANS - b) ORM for Flask applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca43a09-9da5-4835-9228-3bcfc1a55486",
   "metadata": {},
   "source": [
    "##### How do you return a JSON response in Flask?\n",
    "    a) return jsonify(data)\n",
    "    b) return json(data)\n",
    "    c) return as_json(data)\n",
    "    d) return to_json(data)\n",
    "\n",
    "ANS - a) return jsonify(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29d9c34",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd40d261-86ab-4851-a520-8e5e1bcbd187",
   "metadata": {},
   "source": [
    "###### In reinforcement learning, what is an \"agent\"?\n",
    "    a) A dataset\n",
    "    b) A learning algorithm\n",
    "    c) An entity that interacts with the environment\n",
    "    d) A loss function\n",
    "\n",
    "ANS - c) An entity that interacts with the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838e4d3c-590a-4a74-9a3b-ddd43e82e2c1",
   "metadata": {},
   "source": [
    "##### What does \"reward\" in reinforcement learning represent?\n",
    "    a) A measure of performance\n",
    "    b) An optimization function\n",
    "    c) A dataset\n",
    "    d) A probability distribution\n",
    "\n",
    "ANS - a) A measure of performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f650a789-cead-4815-9d4f-98056abc32a1",
   "metadata": {},
   "source": [
    "##### Which algorithm is commonly used in reinforcement learning?\n",
    "    a) Q-learning\n",
    "    b) Linear regression\n",
    "    c) K-means clustering\n",
    "    d) NaÃ¯ve Bayes\n",
    "\n",
    "ANS - a) Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3879fd9f-c91f-4d42-bf64-e4abb05ac9f8",
   "metadata": {},
   "source": [
    "##### What is an \"episode\" in reinforcement learning?\n",
    "    a) A complete sequence of states, actions, and rewards\n",
    "    b) A single step in learning\n",
    "    c) A specific loss function\n",
    "    d) A random decision\n",
    "\n",
    "ANS - a) A complete sequence of states, actions, and rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763b0ca3-fa7e-40c8-b721-8b2db2a18e2f",
   "metadata": {},
   "source": [
    "##### What is the exploration-exploitation tradeoff in RL?\n",
    "    a) Balancing between learning new things and maximizing rewards\n",
    "    b) Choosing between two actions randomly\n",
    "    c) Running two different algorithms\n",
    "    d) Ignoring rewards\n",
    "\n",
    "ANS - a) Balancing between learning new things and maximizing rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7166bf31-3333-4715-83b7-093b002e05da",
   "metadata": {},
   "source": [
    "##### What is \"policy\" in reinforcement learning?\n",
    "    a) A function mapping states to actions\n",
    "    b) A loss function\n",
    "    c) A reward function\n",
    "    d) A dataset\n",
    "\n",
    "ANS - a) A function mapping states to actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff823af",
   "metadata": {},
   "source": [
    "## Unit Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9c8344-6403-4a72-aec1-47ac5ef841b4",
   "metadata": {},
   "source": [
    "##### Which library is commonly used for unit testing in Python?\n",
    "    a) pytest\n",
    "    b) unittest\n",
    "    c) Both a and b\n",
    "    d) None of the above\n",
    "\n",
    "ANS - b) unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c925a1-2c69-4453-8b42-d7c073ad5828",
   "metadata": {},
   "source": [
    "##### What is the main purpose of unit testing?\n",
    "    a) Test entire application at once\n",
    "    b) Validate individual components of a program\n",
    "    c) Check database connectivity\n",
    "    d) Monitor real-time logs\n",
    "\n",
    "ANS -  b) Validate individual components of a program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db48f2dc",
   "metadata": {},
   "source": [
    "## Pylint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066a00f9-e541-4506-9d50-e936ffd4a496",
   "metadata": {},
   "source": [
    "##### What is the purpose of Pylint?\n",
    "    a) Debugging\n",
    "    b) Checking code quality\n",
    "    c) Writing documentation\n",
    "    d) Performance testing\n",
    "\n",
    "ANS - b) Checking code quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929b7bc6",
   "metadata": {},
   "source": [
    "## Apache Spark "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79075b6f-a854-476f-bbb7-57d3a0cf7b92",
   "metadata": {},
   "source": [
    "##### Apache Spark is primarily used for:\n",
    "    a) Web development\n",
    "    b) Distributed data processing\n",
    "    c) Image processing\n",
    "    d) Mobile application development\n",
    "\n",
    "ANS - b) Distributed data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b032b4",
   "metadata": {},
   "source": [
    "## Unit Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0663c659-9882-4184-8ea9-e39bcfb8219e",
   "metadata": {},
   "source": [
    "###### Which of the following is a benefit of unit testing?\n",
    "    a) Reduces debugging time\n",
    "    b) Improves code quality\n",
    "    c) Helps catch errors early\n",
    "    d) All of the above\n",
    "\n",
    "ANS - d) All of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2027a9-28ea-4dcc-96e2-1f3af4c6ecb4",
   "metadata": {},
   "source": [
    "###### In unittest, which method is used to check if two values are equal?\n",
    "    a) assertEqual()\n",
    "    b) assertTrue()\n",
    "    c) assertIs()\n",
    "    d) assertNotEqual()\n",
    "\n",
    "ANS - a) assertEqual()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af2126c-7a92-48d0-9f50-5cd0598f79e6",
   "metadata": {},
   "source": [
    "##### What does pytest use to automatically discover test files?\n",
    "    a) Files starting with test_\n",
    "    b) Files ending with _test.py\n",
    "    c) Both a and b\n",
    "    d) None of the above\n",
    "\n",
    "ANS - c) Both a and b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f18abd0-4bf0-4df9-ac0d-4a3684a3963a",
   "metadata": {},
   "source": [
    "###### How do you run all test cases in pytest?\n",
    "    a) pytest run all\n",
    "    b) pytest\n",
    "    c) pytest test.py\n",
    "    d) python -m pytest\n",
    "\n",
    "ANS - b) pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacc22a9",
   "metadata": {},
   "source": [
    "## Pylint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50e03f5-bb53-413b-86f6-36e1aeb7a47c",
   "metadata": {},
   "source": [
    "##### What is Pylint used for in Python?\n",
    "    a) Detecting syntax errors\n",
    "    b) Enforcing coding standards\n",
    "    c) Both a and b\n",
    "    d) Running performance tests\n",
    "\n",
    "ANS - c) Both a and b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5f997f-3a4a-4e3b-8ceb-7a3310a78029",
   "metadata": {},
   "source": [
    "###### Which command runs Pylint on a Python file?\n",
    "    a) pylint filename.py\n",
    "    b) python -m pylint filename.py\n",
    "    c) Both a and b\n",
    "    d) pylint run filename.py\n",
    "\n",
    "ANS - c) Both a and b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276254fe-3b7e-45f1-b097-a3a3793dd27e",
   "metadata": {},
   "source": [
    "##### Pylint scores code quality on a scale of:\n",
    "    a) 0 to 5\n",
    "    b) 0 to 10\n",
    "    c) -10 to 10\n",
    "    d) 0 to 100\n",
    "\n",
    "ANS - b) 0 to 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d98e2b-5030-4748-bee6-14a6966d6d4e",
   "metadata": {},
   "source": [
    "###### Which of the following is NOT checked by Pylint?\n",
    "    a) Code formatting\n",
    "    b) Logic errors\n",
    "    c) Variable naming conventions\n",
    "    d) Run-time performance\n",
    "\n",
    "ANS - d) Run-time performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7326b4-445f-4ecd-a77a-4bb49dfed682",
   "metadata": {},
   "source": [
    "##### Pylint can be configured using which file format?\n",
    "    a) JSON\n",
    "    b) .pylintrc\n",
    "    c) YAML\n",
    "    d) XML\n",
    "\n",
    "ANS - b) .pylintrc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d9fa43",
   "metadata": {},
   "source": [
    "## Apache Spark "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0968dc5e-2ee9-4667-a10f-54c6785b93db",
   "metadata": {},
   "source": [
    "##### Apache Spark is written in which programming language?\n",
    "    a) Python\n",
    "    b) Java\n",
    "    c) Scala\n",
    "    d) C++\n",
    "\n",
    "ANS - c) Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1f8879-4aeb-4c9f-8018-8e799b3c3235",
   "metadata": {},
   "source": [
    "##### Which component of Apache Spark is used for real-time data processing?\n",
    "    a) Spark SQL\n",
    "    b) Spark Streaming\n",
    "    c) MLlib\n",
    "    d) GraphX\n",
    "\n",
    "ANS - b) Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2be759e-0f63-4b6e-8cfd-86bdb36febe0",
   "metadata": {},
   "source": [
    "###### What is the default cluster manager for Apache Spark?\n",
    "    a) Kubernetes\n",
    "    b) Hadoop YARN\n",
    "    c) Spark Standalone\n",
    "    d) Apache Mesos\n",
    "\n",
    "ANS - c) Spark Standalone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816bd8ec-ff90-437c-a0b9-18ecc382bc25",
   "metadata": {},
   "source": [
    "###### Which API in Apache Spark is used for batch processing?\n",
    "    a) RDD\n",
    "    b) Spark SQL\n",
    "    c) Spark Streaming\n",
    "    d) MLlib\n",
    "\n",
    "ANS - a) RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7aede6-f376-4789-89fd-f32f6f75fa7e",
   "metadata": {},
   "source": [
    "##### What is the primary abstraction in Spark?\n",
    "    a) DataFrame\n",
    "    b) Dataset\n",
    "    c) RDD\n",
    "    d) Table\n",
    "\n",
    "ANS - c) RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ad3497-60d9-4ee9-bced-5d188a24290c",
   "metadata": {},
   "source": [
    "###### What is a DataFrame in Spark?\n",
    "    a) A distributed collection of data organized into named columns\n",
    "    b) A type of database\n",
    "    c) A visualization tool\n",
    "    d) A low-level API for cluster computing\n",
    "\n",
    "ANS - a) A distributed collection of data organized into named columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ec0385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5603ebed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b9e19e7",
   "metadata": {},
   "source": [
    "# Apache Spark - Classification Model Assignment\n",
    "\n",
    "## Question: You are required to generate a dataset using Apache Spark's StructType and StructField. The dataset should contain at least 1000 rows and have the following features:\n",
    "\n",
    "    age (Integer)\n",
    "    salary (Float)\n",
    "    experience (Integer)\n",
    "    education_level (String: [\"High School\", \"Bachelor\", \"Master\", \"PhD\"])\n",
    "    job_role (String: [\"Engineer\", \"Manager\", \"Analyst\", \"Clerk\"])\n",
    "    hired (Label - Integer: 1 if hired, 0 if not hired)\n",
    "\n",
    "\n",
    "\n",
    "    Tasks:\n",
    "    Generate a synthetic dataset using Apache Spark with the above schema.\n",
    "    Use VectorAssembler to convert features into a format suitable for MLlib.\n",
    "    Split the dataset into training (80%) and testing (20%) sets.\n",
    "    Train a classification model (Logistic Regression, Decision Tree, or Random Forest).\n",
    "    Evaluate the model using accuracy and F1-score.\n",
    "    Display the predictions on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74dbd5c",
   "metadata": {},
   "source": [
    "## Example Code for Dataset Generation & Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc3636b1-f4a7-4d3e-9902-691c1eacd1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (3.5.4)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21644f98-d1c7-40f7-846d-5eefa3c9ad26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (3.5.4)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "256a623d-101b-4eed-8b2e-3be2454634d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (3.5.4)\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark findspark pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33e2e723-d15d-494f-bde3-284722936f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.4\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a194a3ac-8c15-4330-90a8-536b300f528a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\r\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\r\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:577)\r\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2416)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2416)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:329)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1575)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize Spark Session\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkClassification\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Define schema\u001b[39;00m\n\u001b[0;32m     12\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[0;32m     13\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     14\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msalary\u001b[39m\u001b[38;5;124m\"\u001b[39m, FloatType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhired\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m ])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         SparkContext(conf\u001b[38;5;241m=\u001b[39mconf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[0;32m    206\u001b[0m         sparkHome,\n\u001b[0;32m    207\u001b[0m         pyFiles,\n\u001b[0;32m    208\u001b[0m         environment,\n\u001b[0;32m    209\u001b[0m         batchSize,\n\u001b[0;32m    210\u001b[0m         serializer,\n\u001b[0;32m    211\u001b[0m         conf,\n\u001b[0;32m    212\u001b[0m         jsc,\n\u001b[0;32m    213\u001b[0m         profiler_cls,\n\u001b[0;32m    214\u001b[0m         udf_profiler_cls,\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_context(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf\u001b[38;5;241m.\u001b[39m_jconf)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mJavaSparkContext(jconf)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\py4j\\java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1588\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fqn)\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\r\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\r\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:577)\r\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2416)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2416)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:329)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1575)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import random\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"SparkClassification\").getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", FloatType(), True),\n",
    "    StructField(\"experience\", IntegerType(), True),\n",
    "    StructField(\"education_level\", StringType(), True),\n",
    "    StructField(\"job_role\", StringType(), True),\n",
    "    StructField(\"hired\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Generate synthetic data\n",
    "data = []\n",
    "education_levels = [\"High School\", \"Bachelor\", \"Master\", \"PhD\"]\n",
    "job_roles = [\"Engineer\", \"Manager\", \"Analyst\", \"Clerk\"]\n",
    "\n",
    "for _ in range(1000):\n",
    "    age = random.randint(22, 60)\n",
    "    salary = round(random.uniform(30000, 120000), 2)\n",
    "    experience = random.randint(0, 30)\n",
    "    education_level = random.choice(education_levels)\n",
    "    job_role = random.choice(job_roles)\n",
    "    hired = 1 if (experience > 5 and salary > 50000) else 0\n",
    "    data.append((age, salary, experience, education_level, job_role, hired))\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff83e88-94e4-46b4-ac5c-6fb45dc6d3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
